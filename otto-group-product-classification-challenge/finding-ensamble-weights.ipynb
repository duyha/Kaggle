{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/hsperr/otto-group-product-classification-challenge/finding-ensamble-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 61878 rows and 95 columns\n",
      "   feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
      "0       1       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       1       0   \n",
      "2       0       0       0       0       0       0       0       1       0   \n",
      "3       1       0       0       1       6       1       5       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   feat_10   ...     feat_84  feat_85  feat_86  feat_87  feat_88  feat_89  \\\n",
      "0        0   ...           0        1        0        0        0        0   \n",
      "1        0   ...           0        0        0        0        0        0   \n",
      "2        0   ...           0        0        0        0        0        0   \n",
      "3        1   ...          22        0        1        2        0        0   \n",
      "4        0   ...           0        1        0        0        0        0   \n",
      "\n",
      "   feat_90  feat_91  feat_92  feat_93  \n",
      "0        0        0        0        0  \n",
      "1        0        0        0        0  \n",
      "2        0        0        0        0  \n",
      "3        0        0        0        0  \n",
      "4        1        0        0        0  \n",
      "\n",
      "[5 rows x 93 columns]\n",
      "RFC LogLoss 0.685110215121\n",
      "LogisticRegression LogLoss 0.672483590758\n",
      "RFC2 LogLoss 0.65415582645\n",
      "Ensamble Score: 0.563621629305\n",
      "Best Weights: [ 0.42491831  0.13610315  0.43897854]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import os\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "print(\"Training set has {0[0]} rows and {0[1]} columns\".format(train.shape))\n",
    "\n",
    "labels = train['target']\n",
    "train.drop(['target', 'id'], axis=1, inplace=True)\n",
    "\n",
    "print(train.head())\n",
    "\n",
    "### we need a test set that we didn't train on to find the best weights for combining the classifiers\n",
    "sss = StratifiedShuffleSplit(labels, test_size=0.05, random_state=1234)\n",
    "for train_index, test_index in sss:\n",
    "    break\n",
    "\n",
    "train_x, train_y = train.values[train_index], labels.values[train_index]\n",
    "test_x, test_y = train.values[test_index], labels.values[test_index]\n",
    "\n",
    "### building the classifiers\n",
    "clfs = []\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=4141, n_jobs=-1)\n",
    "rfc.fit(train_x, train_y)\n",
    "print('RFC LogLoss {score}'.format(score=log_loss(test_y, rfc.predict_proba(test_x))))\n",
    "clfs.append(rfc)\n",
    "\n",
    "### usually you'd use xgboost and neural nets here\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_x, train_y)\n",
    "print('LogisticRegression LogLoss {score}'.format(score=log_loss(test_y, logreg.predict_proba(test_x))))\n",
    "clfs.append(logreg)\n",
    "\n",
    "rfc2 = RandomForestClassifier(n_estimators=50, random_state=1337, n_jobs=-1)\n",
    "rfc2.fit(train_x, train_y)\n",
    "print('RFC2 LogLoss {score}'.format(score=log_loss(test_y, rfc2.predict_proba(test_x))))\n",
    "clfs.append(rfc2)\n",
    "\n",
    "\n",
    "### finding the optimum weights\n",
    "\n",
    "predictions = []\n",
    "for clf in clfs:\n",
    "    predictions.append(clf.predict_proba(test_x))\n",
    "\n",
    "def log_loss_func(weights):\n",
    "    ''' scipy minimize will pass the weights as a numpy array '''\n",
    "    final_prediction = 0\n",
    "    for weight, prediction in zip(weights, predictions):\n",
    "            final_prediction += weight*prediction\n",
    "\n",
    "    return log_loss(test_y, final_prediction)\n",
    "    \n",
    "#the algorithms need a starting value, right not we chose 0.5 for all weights\n",
    "#its better to choose many random starting points and run minimize a few times\n",
    "starting_values = [0.5]*len(predictions)\n",
    "\n",
    "#adding constraints  and a different solver as suggested by user 16universe\n",
    "#https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n",
    "cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "#our weights are bound between 0 and 1\n",
    "bounds = [(0,1)]*len(predictions)\n",
    "\n",
    "res = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "\n",
    "print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n",
    "print('Best Weights: {weights}'.format(weights=res['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-a28da5c4fce5>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-a28da5c4fce5>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This script creates a sample submission using Random Forests\n",
    "# and also plots the feature importance from the trained model.\n",
    "#\n",
    "# To submit the sample, download 1_random_forest_benchmark.csv.gz\n",
    "# from the Output tab and submit it as normal to the competition\n",
    "# (through https://www.kaggle.com/c/otto-group-product-classification-challenge/submissions/attach)\n",
    "#\n",
    "# Click \"fork\" to run this script yourself and make tweaks,\n",
    "# there's many ways you can improve on this!\n",
    "\n",
    "library(ggplot2)\n",
    "library(randomForest)\n",
    "library(readr)\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "train <- read_csv(\"train.csv\")\n",
    "test  <- read_csv(\"test.csv\")\n",
    "\n",
    "submission <- data.frame(id=test$id, Class_1=NA, Class_2=NA, Class_3=NA, Class_4=NA, Class_5=NA, Class_6=NA, Class_7=NA, Class_8=NA, Class_9=NA)\n",
    "\n",
    "rf <- randomForest(train[,c(-1,-95)], as.factor(train$target), ntree=25, importance=TRUE)\n",
    "submission[,2:10] <- (predict(rf, test[,-1], type=\"prob\")+0.01)/1.09\n",
    "\n",
    "gz_out <- gzfile(\"1_random_forest_benchmark.csv.gz\", \"w\")\n",
    "writeChar(write_csv(submission, \"\"), gz_out, eos=NULL)\n",
    "close(gz_out)\n",
    "\n",
    "imp <- importance(rf, type=1)\n",
    "featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "p <- ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "  geom_bar(stat=\"identity\", fill=\"#53cfff\") +\n",
    "  coord_flip() + \n",
    "  theme_light(base_size=20) +\n",
    "  xlab(\"Importance\") +\n",
    "  ylab(\"\") + \n",
    "  ggtitle(\"Random Forest Feature Importance\\n\") +\n",
    "  theme(plot.title=element_text(size=18))\n",
    "\n",
    "ggsave(\"2_feature_importance.png\", p, height=20, width=8, units=\"in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
